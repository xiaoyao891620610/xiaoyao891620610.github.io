<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://xiaoyao891620610.github.io</id>
    <title>萧瑟</title>
    <updated>2021-09-28T14:59:33.084Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://xiaoyao891620610.github.io"/>
    <link rel="self" href="https://xiaoyao891620610.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://xiaoyao891620610.github.io/images/avatar.png</logo>
    <icon>https://xiaoyao891620610.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, 萧瑟</rights>
    <entry>
        <title type="html"><![CDATA[MapReduce详解和面临问题的解决方案]]></title>
        <id>https://xiaoyao891620610.github.io/post/mapreduce-xiang-jie-he-mian-lin-wen-ti-de-jie-jue-fang-an/</id>
        <link href="https://xiaoyao891620610.github.io/post/mapreduce-xiang-jie-he-mian-lin-wen-ti-de-jie-jue-fang-an/">
        </link>
        <updated>2021-09-27T13:37:34.000Z</updated>
        <content type="html"><![CDATA[<h3 id="1mapreduce的各个执行阶段">1.MapReduce的各个执行阶段</h3>
<p>​	我们了解了MapReduce的计算模型是由三个阶段组成：map，shuffle，reduce。</p>
<p>​		Map是映射，负责数据的过滤，通过读取hdfs上的文件block进行切片(inputsplit)，读取记录( recordReader),mapper,combiner(可选)，partitioner分区。这几个阶段对数据进行分析过滤然后通过网络拷贝到reduce阶段。</p>
<p>​		Reduce是归约，通过网络拷贝处理map的结果，混排(shuffle)，排序(sort)，reducer，输出(output)。</p>
<p>官方示意图：</p>
<p>​	<img src="https://xiaoyao891620610.github.io/post-images/1632800978389.png" alt="" loading="lazy"></p>
<p>​</p>
<p>​		官方给的MapReduce的工作流程大概是明白map到reduce阶段所要执行的操作，但深入了解的话还是不够的，所以我们写了一个类似开发语言的&quot;Hello Word&quot;,MapReduce通用明白小案例WordCount，简单来说就是计算一个文件内单词出现的次数，也就是词频。</p>
<p>​		假设文件内的单词是以空格分开，当然其它的也可以，因为可以在代码阶段来设置分割符，内容如下：</p>
<p>​		hello you</p>
<p>​		hello me</p>
<p>​		代码示例：</p>
<pre><code>package main.java.hadoop001.word;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.Iterator;

/**
 * 把hdfs中的hello.txt文件进行wordCount处理
 * hello you
 * hello me
 *
 * 处理后
 * hello 2
 * me   1
 * you 1
 */
public class WordCountJob {
    public static void main(String[] args) {
        try {
            if(args.length != 2) {
                System.exit(100);
            }
            //job需要配置的参数
            Configuration conf = new Configuration();
            Job job = Job.getInstance(conf);
            //设置集群中找寻这个Job类
            job.setJarByClass(WordCountJob.class);
            //指定文件的输出和输入的路径
            FileInputFormat.setInputPaths(job,new Path(args[0]));
            FileOutputFormat.setOutputPath(job,new Path(args[1]));
            //需要指定mapper代码
            job.setMapperClass(MyMapper.class);
            //指定k2的类型
            job.setMapOutputKeyClass(Text.class);
            //指定v2的类型
            job.setMapOutputValueClass(LongWritable.class);

            //需要指定reducer
            job.setReducerClass(MyReducer.class);
            //指定k3的类型
            job.setOutputKeyClass(Text.class);
            //指定v3的类型
            job.setOutputValueClass(MyReducer.class);

            //提交job任务
            job.waitForCompletion(true);
        }catch (Exception e){
            e.printStackTrace();
        }
    }
    public static class MyMapper extends Mapper&lt;LongWritable, Text,Text,LongWritable&gt;{
        Logger logger = LoggerFactory.getLogger(MyMapper.class);
        /**
         *接受&lt;key,value&gt; 产生&lt;k2,v2&gt;
         * @param k1
         * @param v1
         * @param context
         * @throws IOException
         * @throws InterruptedException
         */
        @Override
        protected void map(LongWritable k1, Text v1,Context context)
                throws IOException, InterruptedException {
            logger.info(&quot;&lt;k1,v1&gt;=&gt;&quot;+&quot;&lt;&quot;+k1.get()+&quot;,&quot;+v1.toString()+&quot;&gt;&quot;);
            //k1代表的是每一行首的偏移量，v1代表的是每一行内容 按照空格分割
            String[] words = v1.toString().split(&quot; &quot;);
            //循环字符数组
            for(String word: words){
                Text key2 = new Text(word);
                LongWritable val2 = new LongWritable(1L);
                context.write(key2,val2);
            }
        }
    }
    /**
     * 创建自定义的reducer类
     */
    public static class MyReducer extends Reducer&lt;Text,LongWritable,Text,LongWritable&gt;{
        Logger logger = LoggerFactory.getLogger(MyReducer.class);
        /**
         * 针对字符出现的次数进行求和 最终出去的是&lt;k3,v3&gt;
         * @param key
         * @param values
         * @param context
         * @throws IOException
         * @throws InterruptedException
         */
        @Override
        protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Reducer&lt;Text, LongWritable, Text, LongWritable&gt;.Context context)
                throws IOException, InterruptedException {
            Long sum = 0L;
            Iterator&lt;LongWritable&gt; it = values.iterator();
            while (it.hasNext()){
                sum += it.next().get();
            }
            Text k3 = key;

            LongWritable v3 = new LongWritable(sum);
            logger.info(&quot;&lt;k1,v1&gt;=&gt;&quot;+&quot;&lt;&quot;+k3.toString()+&quot;,&quot;+v3.get()+&quot;&gt;&quot;);

            context.write(k3,v3);
        }
    }
}

</code></pre>
<p>最终执行的结果是：</p>
<p>​		hello  2</p>
<p>​		you    1</p>
<p>​		me 	1</p>
<p>代码中的日志可以忽略不计，如果你说，看了WordCount之后还是不明白，那肯定的啊，因为MapReduce整个阶段的切片split，读取记录行和reduce阶段的排序，输出都被进行了封装，我们代码所能实现的就只有map和reduce。</p>
<h3 id="2mapreduce的job作业-input-split-输入分切片解析">2.MapReduce的job作业 -&gt; input split (输入分/切片)解析</h3>
<p>懒得画，盗了个图</p>
<p>​	<img src="https://xiaoyao891620610.github.io/post-images/1632800956166.png" alt="" loading="lazy"></p>
<p>如图所示，hdfs的文件是被切分成block块的，那么在进行map作业之前，mapreduce会根据输入的文件来逻辑计算，划分分片的大小(input split)，那么每个分片split对应的是一个map task任务，因为我这边也说了是逻辑计算分片的大小，所以并不是数据的本身，而是一个分片的长度和一个记录数据位置的数组。</p>
<h4 id="21逻辑切片的计算源码">2.1逻辑切片的计算源码</h4>
<p>​	源码逻辑计算切片代码：</p>
<pre><code>/** 
   * Generate the list of files and make them into FileSplits.
   * @param job the job context
   * @throws IOException
   */
  public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException {
    StopWatch sw = new StopWatch().start();
    //最小切片(split)默认配置
    //SPLIT_MINSIZE=&quot;mapreduce.input.fileinputformat.split.minsize&quot;;
    //调用方法getMinSplitSize(job){
    //return job.getConfiguration().getLong(SPLIT_MINSIZE, 1L)};
    //默认值0L
    //与默认值getFormatMinSplitSize(){return 1;}
    //比较
    //最后取最大值1L
    long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
    //获取最大切片值
    //public static long getMaxSplitSize(JobContext context) {
    //return context.getConfiguration().getLong(SPLIT_MAXSIZE, 
                                              Long.MAX_VALUE);
  	//}
  	//Long.MAX_VALUE 为最大值
    long maxSize = getMaxSplitSize(job);

    // generate splits
    List&lt;InputSplit&gt; splits = new ArrayList&lt;InputSplit&gt;();
    //读取作业输入文件
    List&lt;FileStatus&gt; files = listStatus(job);

    boolean ignoreDirs = !getInputDirRecursive(job)
      &amp;&amp; job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);
    for (FileStatus file: files) {
      if (ignoreDirs &amp;&amp; file.isDirectory()) {
        continue;
      }
      //读取文件路径
      Path path = file.getPath();
      //计算文件长度
      long length = file.getLen();
      if (length != 0) {
        BlockLocation[] blkLocations;
        if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getBlockLocations();
        } else {
          FileSystem fs = path.getFileSystem(job.getConfiguration());
          blkLocations = fs.getFileBlockLocations(file, 0, length);
        }
        if (isSplitable(job, path)) {
          //获取文件block块大小 默认134217728Byte(128M)
          long blockSize = file.getBlockSize();
          //确定最终切片(split)的大小
          //protected long computeSplitSize(long blockSize, long minSize,
          //                       long maxSize) {
    	  //return Math.max(minSize, Math.min(maxSize, blockSize));
  		  //	}
  		  //因为Hadoop2.x之后默认的block值大小为128M，所以最终所得的切片值为128M，也就是134217728Byte
          long splitSize = computeSplitSize(blockSize, minSize, maxSize);
		  //block切片的剩余split大小(Byte)
          long bytesRemaining = length;
          //SPLIT_SLOP 溢出值为1.1L   SPLIT_SLOP = 1.1
          //所以获取的文件最大字节数length除以每个切片的字节splitSize如果大于1.1的话将会循环对文件进行切片
          while (((double) bytesRemaining)/splitSize &gt; SPLIT_SLOP) {
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
            splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                        blkLocations[blkIndex].getHosts(),
                        blkLocations[blkIndex].getCachedHosts()));
            //递减操作
            bytesRemaining -= splitSize;
          }
		  //最终被切片剩下的block存为单独的split
          if (bytesRemaining != 0) {
          	//计算文件分块的索引，计算了spilt位于某个block块中
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                       blkLocations[blkIndex].getHosts(),
                       blkLocations[blkIndex].getCachedHosts()));
          }
        } else { // not splitable
          if (LOG.isDebugEnabled()) {
            // Log only if the file is big enough to be splitted
            if (length &gt; Math.min(file.getBlockSize(), minSize)) {
              LOG.debug(&quot;File is not splittable so no parallelization &quot;
                  + &quot;is possible: &quot; + file.getPath());
            }
          }
          splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                      blkLocations[0].getCachedHosts()));
        }
      } else { 
        //Create empty hosts array for zero length files
        splits.add(makeSplit(path, 0, length, new String[0]));
      }
    }
    // Save the number of input files for metrics/loadgen
    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
    sw.stop();
    if (LOG.isDebugEnabled()) {
      LOG.debug(&quot;Total # of splits generated by getSplits: &quot; + splits.size()
          + &quot;, TimeTaken: &quot; + sw.now(TimeUnit.MILLISECONDS));
    }
    return splits;
  }
</code></pre>
<p>这里我已经将怎么划分split的计算逻辑做了注释，所以当我们在默认的配置下，每个切片的大小约等于blocksize的大小，因为设置的溢出值SPLIT_SLOP是1.1，等于，大于，小于blocksize都是可能的bytesRemaining/splitSize &gt; 1.1)。大概也就是134217728Byte(128M)。</p>
<h4 id="22-recordreader读取split内的行记录">2.2 recordReader读取split内的行记录</h4>
<p>​	先贴上源码</p>
<pre><code>public void initialize(InputSplit genericSplit,
                         TaskAttemptContext context) throws IOException {
    FileSplit split = (FileSplit) genericSplit;
    Configuration job = context.getConfiguration();
    this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);
    //获取split分片的起始偏移量
    start = split.getStart();
    //split分片的大小(Byte)
    end = start + split.getLength();
    //split分片所在的文件路径
    final Path file = split.getPath();

    // open the file and seek to the start of the split
    final FileSystem fs = file.getFileSystem(job);
    fileIn = fs.open(file);
    
    CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);
    //判断是否是压缩文件
    if (null!=codec) {
      isCompressedInput = true;
      decompressor = CodecPool.getDecompressor(codec);
      if (codec instanceof SplittableCompressionCodec) {
        final SplitCompressionInputStream cIn =
          ((SplittableCompressionCodec)codec).createInputStream(
            fileIn, decompressor, start, end,
            SplittableCompressionCodec.READ_MODE.BYBLOCK);
        in = new CompressedSplitLineReader(cIn, job,
            this.recordDelimiterBytes);
        start = cIn.getAdjustedStart();
        end = cIn.getAdjustedEnd();
        filePosition = cIn;
      } else {
        if (start != 0) {
          // So we have a split that is only part of a file stored using
          // a Compression codec that cannot be split.
          throw new IOException(&quot;Cannot seek in &quot; +
              codec.getClass().getSimpleName() + &quot; compressed stream&quot;);
        }

        in = new SplitLineReader(codec.createInputStream(fileIn,
            decompressor), job, this.recordDelimiterBytes);
        filePosition = fileIn;
      }
    } else {
      //正常文件分支 
      fileIn.seek(start); //定位输入文件的起始位置
      in = new UncompressedSplitLineReader(
          fileIn, job, this.recordDelimiterBytes, split.getLength());
      filePosition = fileIn;
    }
    // If this is not the first split, we always throw away first record
    // because we always (except the last split) read one extra line in
    // next() method.
    //判断是否是属于第一个split的起始位置0，如果不是那么会忽略掉第一行的数据
    if (start != 0) {
      start += in.readLine(new Text(), 0, maxBytesToConsume(start));
    }
    this.pos = start;
  }
  public boolean nextKeyValue() throws IOException {
    if (key == null) {
      key = new LongWritable();
    }
    key.set(pos);
    if (value == null) {
      value = new Text();
    }
    int newSize = 0;
    // We always read one extra line, which lies outside the upper
    // split limit i.e. (end - 1)
    //判断当前读取行记录的偏移量小于当前split &amp; 当前读取已经到下一个split的时候继续读取，也就是说总会读取下一行，或者读取下一个split的第一行，然后重新赋值行读取所在的偏移量pos
    while (getFilePosition() &lt;= end || in.needAdditionalRecordAfterSplit()) {
      if (pos == 0) {
        newSize = skipUtfByteOrderMark();
      } else {
        newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));
        pos += newSize;
      }

      if ((newSize == 0) || (newSize &lt; maxLineLength)) {
        break;
      }

      // line too long. try again
      LOG.info(&quot;Skipped line of size &quot; + newSize + &quot; at pos &quot; + 
               (pos - newSize));
    }
    if (newSize == 0) {
      key = null;
      value = null;
      return false;
    } else {
      return true;
    }
  }
</code></pre>
<p>因为每个block块我们是按照逻辑切片(split)的，通俗明白点就是文件内的字节偏移量。所以我们面临的一个问题就是，当一行数据被切分到两个split中，是怎么不会读取重复或者读取少记录的。</p>
<p>首先解决的是判断当前偏移量是否是为0，或者可以说是不是在第一个split片当中，如果不是，那么行读取器会往下一行开始读取，也就是忽略掉第一行。</p>
<pre><code>if (start != 0) {
   start += in.readLine(new Text(), 0, maxBytesToConsume(start));
}
</code></pre>
<p>行阅读器会判断当前偏移量和当前的所属分片split，默认读取下一个split的第一行</p>
<pre><code> // We always read one extra line, which lies outside the upper
 // split limit i.e. (end - 1)
 //判断当前读取行记录的偏移量小于当前split &amp; 当前读取已经到下一个split的时候继续读取，也就是说总会读取下一行，或者读取下一个split的第一行，然后重新赋值行读取所在的偏移量pos
 while (getFilePosition() &lt;= end || in.needAdditionalRecordAfterSplit()) {
      if (pos == 0) {
        newSize = skipUtfByteOrderMark();
      } else {
        newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));
        pos += newSize;
      }

      if ((newSize == 0) || (newSize &lt; maxLineLength)) {
        break;
      }

      // line too long. try again
      LOG.info(&quot;Skipped line of size &quot; + newSize + &quot; at pos &quot; + 
               (pos - newSize));
    }
</code></pre>
<p>结论：readLine()的功能是读取一行的数据到Text中，因为在分割FileSplit时是基于size的，如果一行被分割到两个split中，比如s1和s2中，在读取s1中的最后一行数据时，会一直读取到s2中的第一个换行符，这是在nextKeyValue()方法中实现的，而在处理s2时，则要将已经读取的数据跳过以避免重复读取，也就是直接开始读取s2的第二行，这是在构造方法中initialize实现的。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[HDFS]]></title>
        <id>https://xiaoyao891620610.github.io/post/hdfs/</id>
        <link href="https://xiaoyao891620610.github.io/post/hdfs/">
        </link>
        <updated>2021-09-23T02:07:47.000Z</updated>
        <content type="html"><![CDATA[<h3 id="1hdfs的概念">1.HDFS的概念</h3>
<p>​	HDFS (Hadoop Distributed File System),是一个分布式文件系统，运用了分而治之的设计思想，部署在低廉的硬件上，将大文件，大批量文件，分布式存放在大量服务器上，并对海量数据进行运算分析。</p>
<pre><code>大数据系统中的作用：为各类分布式的运算框架 (mapreduce,spark,tez,.......)提供数据存储服务
</code></pre>
<h3 id="2hdfs的架构">2.HDFS的架构</h3>
<p>​	HDFS集群中包含了NameNode, SecondaryNameNode,DataNode这几个角色。</p>
<p>​		NameNode(NN)： NameNode是HDFS的主节点，主要用于维护hdfs文件系统的目录树，维护的信息是保存在本地磁盘上：命名空间镜像文件(fsimage)和编辑日志文件(edits)。并且记录这每一个块(block)所在的数据节点(DataNode)的位置(block的id，block所在的datanode服务器)。</p>
<p>​		SecondaryNameNode(SNN)：定期将编辑日志文件(edits)和命名空间镜像文件(fsimage)合并，防止编辑日志文件过大，同时能元数据和namenode元数据一致。</p>
<blockquote>
<p>​		NN和SNN的工作机制</p>
</blockquote>
<p>​		<img src="https://xiaoyao891620610.github.io/post-images/1632383275847.jpg" alt="" loading="lazy"></p>
<p>​		SNN会定期询问NN是否需要checkpoint然后直接带回NN是否检查的结果，当checkpoint定时时间到了或者edits中的数据满了，SNN会请求执行checkpoint。</p>
<p>​		SNN用来完成edits和fsimage的合并，会请求namenode停止使用edits，暂时将写入操作放入一个新的文件edits.new, SecondaryNameNode通过http get请求获取edits和fsimage，同时加载到内存当中逐一进行合并操作，生成新的fsimage.ckpt。然后将fsimage.ckpt通过http post发送给namenode。namenode将fsimage.ckpt文件替换原有的fsimage,再将edits.new替换成edits,同时会更新fstime。</p>
<p>合并触发条件：</p>
<pre><code>fs.checkpoint.period: 默认是一个小时（3600s)
fs.checkpoint.size: edits达到一定大小时也会触发合并（默认64MB) 
</code></pre>
<p>​		DataNode(DN)：是hdfs文件系统的工作节点，以数据块的形式存储hdfs文件，并且定期向NameNode发送心跳信息，数据块信息，缓存数据块信息。</p>
<blockquote>
<p>​		SNN和DN的工作机制</p>
</blockquote>
<p>​		<img src="https://xiaoyao891620610.github.io/post-images/1632445946052.jpg" alt="" loading="lazy"></p>
<ul>
<li>
<p>一个数据块在datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。</p>
</li>
<li>
<p>DataNode启动后向namenode注册，通过后，周期性（1小时）的向namenode上报所有的块信息。</p>
</li>
<li>
<p>心跳是每3秒一次，心跳返回结果带有namenode给该datanode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个datanode的心跳，则认为该节点不可用。</p>
<h2 id="3hdfs的核心概念">3.HDFS的核心概念</h2>
<h3 id="31-blocks">3.1 blocks</h3>
<p>​	磁盘中有块的概念，意味着可以读写的最小数据量。文件系统的大小一般是磁盘的大小的整数倍来运作这个磁盘，一般为几千字节，而磁盘块一般是512Byte。 Hadoop提供的df、fsck这类运维工具都是在文件系统的Block级别上进行操作。</p>
<p>​	HDFS的block块比一般单机文件系统大的多，Hadoop1.x 默认block分块是64M，Hadoop2.x默认是128M。经过测试在上传了2M的文件并非占用了整个block块，而是占用了实际大小，也就是在hdfs当中只用了2M的空间，而不是128M。</p>
<h5 id="block块大小的设计原则">block块大小的设计原则</h5>
<h6 id="1减少磁盘的寻址时间控制寻址和传输文件所用的时间比例">1.减少磁盘的寻址时间：控制寻址和传输文件所用的时间比例。</h6>
<p>​		一般hdfs的寻址时间为10ms，经过反复测试，当寻址时间为传输时间的**1%**时为最佳传输状态。那么一般的机械硬盘文件读写的速度一般为100M/s，200M/s或者 普通固态为500MB/s,pcie固态的速度可以达到2000MB/s,因此块的大小可以分别设为128MB,256MB,512MB,2048MB 。<br>
​<br>
​         最佳传输时间为：10ms/0.01=1000ms=1s<br>
​			计算出最佳block大小：100MB/s x 1s = 100MB<br>
​			所以设定block大小一般为128MB。<br>
​			经过测试我们现阶段可以说，<strong>block的大小主要取决于磁盘传输速率</strong>。</p>
<h6 id="2减少namenode内存消耗因为namenode主节点主要维护的是hdfs所有文件的目录树文件信息-block块太小那么需要需要维护的数据块信息会更多而namenode节点的内存是极其有限的-block块太小则会增加寻址时间导致程序一直在寻找block的起始位置">2.减少namenode内存消耗：因为namenode主节点主要维护的是hdfs所有文件的目录树，文件信息。block块太小，那么需要需要维护的数据块信息会更多，而namenode节点的内存是极其有限的。block块太小，则会增加寻址时间，导致程序一直在寻找block的起始位置。</h6>
<h6 id="3map崩溃当开启map任务时map-task的数据来源是block当然在mapreduce的概念中是读取split切片split和block默认是一对一-当block过大会导致在启动时数据加载时间越长恢复时间越长">3.map崩溃：当开启map任务时，map task的数据来源是block，当然在mapreduce的概念中是读取split(切片)，split和block默认是一对一。当block过大会导致在启动时数据加载时间越长，恢复时间越长。</h6>
<h6 id="4监管时间问题主节点监管其他节点的情况每个节点会周期性的把完成的工作和状态的更新报告回来-若某个节点保存沉默超过预设的时间间隔主节点宣告该节点状态为死亡并把分配给这个节点的数据发到别的节点-预设的时间间隔是根据数据块-size角度估算的若size设置不合理容易误判节点死亡">4.监管时间问题：主节点监管其他节点的情况，每个节点会周期性的把完成的工作和状态的更新报告回来。若某个节点保存沉默超过预设的时间间隔，主节点“宣告”该节点状态为死亡，并把分配给这个节点的数据发到别的节点。预设的时间间隔是根据数据块 size角度估算的，若size设置不合理，容易误判节点死亡。</h6>
<h6 id="5网络传输问题-在数据读写计算的时候需要进行网络传输如果block过大会导致网络传输时间增长程序卡顿超时无响应-任务执行的过程中拉取其他节点的block或者失败重试的成本会过高如果block过小则会频繁的进行文件传输对严重占用网络cpu资源">5.网络传输问题: 在数据读写计算的时候,需要进行网络传输.如果block过大会导致网络传输时间增长,程序卡顿/超时/无响应. 任务执行的过程中拉取其他节点的block或者失败重试的成本会过高.如果block过小,则会频繁的进行文件传输,对严重占用网络/CPU资源.</h6>
<h3 id="32-namenode-datanode">3.2 Namenode &amp; Datanode</h3>
<p>​	 整个HDFS集群由Namenode和Datanode构成master-worker（主从）模式。Namenode负责构建命名空间，管理文件的元数据等，而Datanode负责实际存储数据，负责读写工作</p>
<h5 id="namenode">Namenode</h5>
<p>Namenode存放文件系统树及所有文件、目录的元数据。元数据持久化为2种形式：</p>
<ul>
<li>
<p>fsimage</p>
</li>
<li>
<p>edits</p>
</li>
</ul>
<p>但是持久化数据中不包括Block所在的节点列表，及文件的Block分布在集群中的哪些节点上，这些信息是在系统重启的时候重新构建（通过Datanode汇报的Block信息）。<br>
在HDFS中，Namenode可能成为集群的单点故障，Namenode不可用时，整个文件系统是不可用的。HDFS针对单点故障提供了2种解决机制：</p>
<h6 id="1备份持久化元数据">1.备份持久化元数据</h6>
<p>将文件系统的元数据同时写到多个文件系统， 例如同时将元数据写到本地文件系统及NFS。这些备份操作都是同步的、原子的。</p>
<h6 id="2secondary-namenode">2.Secondary Namenode</h6>
<p>Secondary节点定期合并主Namenode的fsimage和edits， 避免edit log过大，通过创建检查点checkpoint来合并。它会维护一个合并后的fsimage副本， 可用于在Namenode完全崩溃时恢复数据。</p>
<p>Secondary Namenode通常运行在另一台机器，因为合并操作需要耗费大量的CPU和内存。其数据落后于Namenode，因此当Namenode完全崩溃时，会出现数据丢失。 通常做法是拷贝NFS中的备份元数据到Second，将其作为新的主Namenode。<br>
在HA（High Availability高可用性）中可以运行一个Hot Standby，作为热备份，在Active Namenode故障之后，替代原有Namenode成为Active Namenode。</p>
<h5 id="datanode">Datanode</h5>
<p>数据节点负责存储和提取Block，读写请求可能来自namenode，也可能直接来自客户端。数据节点周期性向Namenode汇报自己节点上所存储的Block相关信息。</p>
<h3 id="33-block-caching">3.3 Block Caching</h3>
<p>DataNode通常直接从磁盘读取数据，但是频繁使用的Block可以在内存中缓存。默认情况下，一个Block只有一个数据节点会缓存。但是可以针对每个文件可以个性化配置。<br>
作业调度器可以利用缓存提升性能，例如MapReduce可以把任务运行在有Block缓存的节点上。<br>
用户或者应用可以向NameNode发送缓存指令（缓存哪个文件，缓存多久）， 缓存池的概念用于管理一组缓存的权限和资源。</p>
<h3 id="34-hdfs-federation">3.4 HDFS Federation</h3>
<p>我们知道NameNode的内存会制约文件数量，HDFS Federation提供了一种横向扩展NameNode的方式。在Federation模式中，每个NameNode管理命名空间的一部分，例如一个NameNode管理/user目录下的文件， 另一个NameNode管理/share目录下的文件。<br>
每个NameNode管理一个namespace volumn，所有volumn构成文件系统的元数据。每个NameNode同时维护一个Block Pool，保存Block的节点映射等信息。各NameNode之间是独立的，一个节点的失败不会导致其他节点管理的文件不可用。<br>
客户端使用mount table将文件路径映射到NameNode。mount table是在Namenode群组之上封装了一层，这一层也是一个Hadoop文件系统的实现，通过viewfs:协议访问。</p>
<h3 id="35-hdfs-hahigh-availability高可用性">3.5 HDFS HA(High Availability高可用性)</h3>
<p>在HDFS集群中，NameNode依然是单点故障（SPOF: Single Point Of Failure）。元数据同时写到多个文件系统以及Second NameNode定期checkpoint有利于保护数据丢失，但是并不能提高可用性。<br>
这是因为NameNode是唯一一个对文件元数据和file-block映射负责的地方， 当它挂了之后，包括MapReduce在内的作业都无法进行读写。</p>
<p>当NameNode故障时，常规的做法是使用元数据备份重新启动一个NameNode。元数据备份可能来源于：</p>
<ul>
<li>多文件系统写入中的备份</li>
<li>Second NameNode的检查点文件</li>
</ul>
<p>启动新的Namenode之后，需要重新配置客户端和DataNode的NameNode信息。另外重启耗时一般比较久，稍具规模的集群重启经常需要几十分钟甚至数小时，造成重启耗时的原因大致有：<br>
1） 元数据镜像文件载入到内存耗时较长。<br>
2） 需要重放edit log<br>
3） 需要收到来自DataNode的状态报告并且满足条件后才能离开安全模式提供写服务。</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[JAVA-向上转型和向下转型]]></title>
        <id>https://xiaoyao891620610.github.io/post/java-xiang-shang-zhuan-xing-he-xiang-xia-zhuan-xing/</id>
        <link href="https://xiaoyao891620610.github.io/post/java-xiang-shang-zhuan-xing-he-xiang-xia-zhuan-xing/">
        </link>
        <updated>2021-09-17T09:38:55.000Z</updated>
        <content type="html"><![CDATA[<pre><code>🤡 向上转型: 父类的引用指向子类的对象
</code></pre>
<p>一. 向上转型代码示例分析</p>
<pre><code>class Apple extends Fruit{
    @Override
    public void show() {
    	System.out.println(&quot;This is a Apple!&quot;);
    }
    public void circular(){
    	System.out.println(&quot;circular......!&quot;);
    }
}
public class Banana extends Fruit{
    @Override
    public void show() {
    	System.out.println(&quot;This is a Banana!&quot;);
    }
	public void curved(){
    	System.out.println(&quot;curved....!&quot;);
    }
}

public class MainFruitTest {
    public static void toDo(Fruit fruit){
    	fruit.show();
    }
    public static void main(String[] args) {
        //向上转型-父类的引用指向子类的对象
        Fruit apple = new Apple();
        //向下转型-把子类对象的父类引用赋给子类引用，这里是强制转换
        // Apple appleDown = (Apple)apple;
        //appleDown.circular(); // circular......!
        apple.circular(); //编译报错可以运行。运行报错-&gt;java: 找不到符号 符号:   方法 circular()  位置: 类 org.hadoop.Fruit.  注：apple虽然指向的是子类对象，但丢失了circular()方法
        toDo(apple); // This is a Apple!

        Fruit banana = new Banana();
        toDo(banana); // This is a Banana!
    }
}
</code></pre>
<p>​		Fruit apple = new Apple()，实例化的Apple对象赋值给了父类Fruit对象，这便是典型的向上转型。当向上转型之后，父类引用变量便可以访问子类中继承了(或者说属于)父类的属性和方法，但是不能访问子类独有的属性和方法（java: 找不到符号 符号:   方法 circular()  位置: 类 org.hadoop.Fruit）。</p>
<p>二. 向上转型作用</p>
<pre><code>public static void toDo(Fruit fruit){
	fruit.show();
}
</code></pre>
<p>​		以父类为参数，但是在调用的时候传的是子类的对象，方法toDo()以子类为对象，无论是Apple，还是Banana，都会向上转型成Fruit对象，这样在调用show()方法时，还是子类在调用，就不需要再重载多个toDo()方法了，这样会节约代码量，也正提现了JAVA的抽象编程思想。</p>
<pre><code>🤡向下转型：把子类对象的父类引用赋给子类引用
</code></pre>
<p>一.   向下转型的注意点</p>
<p>​		例 1 ：Fruit apple = new Apple();</p>
<p>​					Apple appleDown = (Apple)apple;</p>
<p>​					正确的向下转型，因为appleDown 指向的是子类对象。</p>
<p>​		例 2 ：Fruit fruit = new Fruit();</p>
<p>​					Apple apple = (Apple)fruit;</p>
<pre><code>运行报错：org.hadoop.Fruit cannot be cast to org.hadoop.Apple
at org.hadoop.MainFruitTest.main
</code></pre>
<p>二.  向下转型的作用</p>
<p>​	向上转型时，作为子类对象的父类引用无法调用除与父类共有的方法和属性。那么此时就可以通过向下转型，把父类作为子类的引用强制转换赋给子类的对象，这样就可以调用子类的全部方法。当然要结合实际出发去转换。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[计算机网络概述]]></title>
        <id>https://xiaoyao891620610.github.io/post/wang-luo-gai-shu-he-linux-xu-ni-ji-yi/</id>
        <link href="https://xiaoyao891620610.github.io/post/wang-luo-gai-shu-he-linux-xu-ni-ji-yi/">
        </link>
        <updated>2020-11-20T07:42:18.000Z</updated>
        <summary type="html"><![CDATA[<p>记录，本章是整个计算机网络的概述，后面会具体有五层协议的体系结构。</p>
<p>互联网的组成<br>
互联网的组成是非常复杂的，但是根据它的工作方式我们可以分成两部分：核心部分和边缘部分<br>
　　１. 边缘部分：由所有连接在互联网上的主机组成。这部分是用户直接使用的，用来进行通信（传送数据、音频或视频）和资源共享。<br>
　　２. 核心部分：有大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的（提供连通性和交换）。<br>
　　两种工作模式的示意图。<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605861195301.jpg" alt="" loading="lazy"><br>
　　<br>
<strong>互联网的边缘部分：</strong><br>
　　是连接在互联网上的所有主机，称为端系统(end system）。可以是笔记本电脑，平板，网络摄像头。拥有者可以是个人，企业，甚至是ISP。<br>
　　其它的概念型的东西不好写太多，自己记住就好了。既然是说计算机那么就要知道主机之间是怎么通信的。<br>
　　１.客户－服务器方式（Clent/Server -- C/S）原理图<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605862370955.jpg" alt="" loading="lazy"><br>
　　２．对等连接方式（Peer-to-Peer -- P2P）原理图<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605948572968.png" alt="" loading="lazy"><br>
　　其实P2P的本质还是C/S，只是对等连接中的每一台主机既是客户又是服务器。如图所示可以看出来客户在请求别的服务器的同时也提供着服务。那么这种主机的通信方式有点非常的明显，例如：<strong>快播</strong>。一个节点损坏不影响其它节点继续下载电影。我们可以通过很多的节点去下载1G的电影的小片段，最后重组在一起。所以是下载的人越多，下载的就越快，压根不存在带宽不够的情况。所以这种情况服务器得省很多钱，没必要花大价钱加带宽了。</p>
]]></summary>
        <content type="html"><![CDATA[<p>记录，本章是整个计算机网络的概述，后面会具体有五层协议的体系结构。</p>
<p>互联网的组成<br>
互联网的组成是非常复杂的，但是根据它的工作方式我们可以分成两部分：核心部分和边缘部分<br>
　　１. 边缘部分：由所有连接在互联网上的主机组成。这部分是用户直接使用的，用来进行通信（传送数据、音频或视频）和资源共享。<br>
　　２. 核心部分：有大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的（提供连通性和交换）。<br>
　　两种工作模式的示意图。<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605861195301.jpg" alt="" loading="lazy"><br>
　　<br>
<strong>互联网的边缘部分：</strong><br>
　　是连接在互联网上的所有主机，称为端系统(end system）。可以是笔记本电脑，平板，网络摄像头。拥有者可以是个人，企业，甚至是ISP。<br>
　　其它的概念型的东西不好写太多，自己记住就好了。既然是说计算机那么就要知道主机之间是怎么通信的。<br>
　　１.客户－服务器方式（Clent/Server -- C/S）原理图<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605862370955.jpg" alt="" loading="lazy"><br>
　　２．对等连接方式（Peer-to-Peer -- P2P）原理图<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605948572968.png" alt="" loading="lazy"><br>
　　其实P2P的本质还是C/S，只是对等连接中的每一台主机既是客户又是服务器。如图所示可以看出来客户在请求别的服务器的同时也提供着服务。那么这种主机的通信方式有点非常的明显，例如：<strong>快播</strong>。一个节点损坏不影响其它节点继续下载电影。我们可以通过很多的节点去下载1G的电影的小片段，最后重组在一起。所以是下载的人越多，下载的就越快，压根不存在带宽不够的情况。所以这种情况服务器得省很多钱，没必要花大价钱加带宽了。</p>
<!-- more -->
<!-- more -->
<p><strong>互联网的核心部分：</strong><br>
　　电路交换（Circuit Switching）:例：打电话  建立连接（申请占用通信资源）-&gt;通话（一直占用通信资源）-&gt;释放连接（释放通信资源），电路交换适用于数据量很大的实时性传输：核心路由器之间可以使用电路交换。<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605948188035.jpg" alt="" loading="lazy"><br>
　　　　　　　　　　　　　　　　　　　电路交换示意图</p>
<p>​　　报文交换（Message Switcing）：报文一般比分组长的多，报文交换的时延较长<br>
​　　分组交换（Packet Switcing）：采用存储转发技术，是把一个报文拆分为几个分组后再进行传送。优点：高效，灵活，迅速，可靠  问题：时延，开销<br>
<img src="https://xiaoyao891620610.github.io/post-images/1605948123122.jpg" alt="" loading="lazy"><br>
　　　　　　　　　　　　　　　　　以分组为基本单位在网络中传输</p>
<p><img src="https://xiaoyao891620610.github.io/post-images/1605948852329.jpg" alt="" loading="lazy"><br>
　　　　　　　　　　　　　　　　　三种交换方式的比较</p>
<p>计算机网络的体系结构<br>
　　具有五层协议的体系结构：<br>
　　应用层（Application）：通过应用程序间的交互来完成特定的网络任务。应用层交互的数据单元成为报文（message）。应用层协议：HTTP,SMTP,DNS。<br>
　　运输层（Transport）：两台主机中进程之间的通信提供通用的数据传输服务。运输层主要使用两种协议，TCP协议和UDP协议。<br>
　　×传输控制协议TCP——提供面向连接的，可靠的数据传输服务，单位是报文段。<br>
　　×用户数据包协议ＵDP——提供无连接的，尽最大努力的数据传输服务，不保证数据传输的可靠性，单位是用户数据报。<br>
　　网络层（Network）:为分组交换网上的不同主机提供通信服务。比如IP地址选址，选择最佳路径。<br>
　　数据链路层（Data Link）： 数据如何封装  添加物理层地址（MAC地址）<br>
　　​物理层（Physical）：电压，接口标准（比如网线）<br>
<img src="https://xiaoyao891620610.github.io/post-images/1606047565974.jpg" alt="" loading="lazy"></p>
<p></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[你尽力了吗? (来自中国著名网络安全专家小四scz)]]></title>
        <id>https://xiaoyao891620610.github.io/post/ni-jin-li-liao-ma-lai-zi-zhong-guo-zhu-ming-wang-luo-an-quan-zhuan-jia-xiao-si-scz/</id>
        <link href="https://xiaoyao891620610.github.io/post/ni-jin-li-liao-ma-lai-zi-zhong-guo-zhu-ming-wang-luo-an-quan-zhuan-jia-xiao-si-scz/">
        </link>
        <updated>2020-11-13T01:54:42.000Z</updated>
        <summary type="html"><![CDATA[<!-- more -->
<p>很多人问如何入门如何入门，我却不知道要问的是入什么门。 很少把某些好文章耐心从头看完，我这次就深有体会。比如袁哥的sniffer原理，一直以为自己对sniffer原理很清楚的，所以也就不曾仔细看过袁哥的这篇。后来有天晚上和袁哥讨论，如何通过端口读写直接获取mac地址，为什么antisniff可以获得真正的mac地址，而不受更改mac地址技术的影响，如何在linux下获得真正的mac地址。我一直对linux下的端口读写心存疑虑，总觉得在保护模式下的端口都做 了内存映象等等。结果袁哥问了我一句，你仔细看我写的文章没有，我楞，最近因为要印刷月刊，我整 理以前的很多文档，被迫认真过滤它们，才发现袁哥的文章让我又有新认识。再后来整理到tt的几篇缓 冲区溢出的，尤其是上面的关于Solaris可装载内核模块，那就更觉得惭愧了。 以前说书非借不能读，现在是文章留在硬盘上却不读。</p>
]]></summary>
        <content type="html"><![CDATA[<!-- more -->
<p>很多人问如何入门如何入门，我却不知道要问的是入什么门。 很少把某些好文章耐心从头看完，我这次就深有体会。比如袁哥的sniffer原理，一直以为自己对sniffer原理很清楚的，所以也就不曾仔细看过袁哥的这篇。后来有天晚上和袁哥讨论，如何通过端口读写直接获取mac地址，为什么antisniff可以获得真正的mac地址，而不受更改mac地址技术的影响，如何在linux下获得真正的mac地址。我一直对linux下的端口读写心存疑虑，总觉得在保护模式下的端口都做 了内存映象等等。结果袁哥问了我一句，你仔细看我写的文章没有，我楞，最近因为要印刷月刊，我整 理以前的很多文档，被迫认真过滤它们，才发现袁哥的文章让我又有新认识。再后来整理到tt的几篇缓 冲区溢出的，尤其是上面的关于Solaris可装载内核模块，那就更觉得惭愧了。 以前说书非借不能读，现在是文章留在硬盘上却不读。</p>
<!-- more -->
<p>其实本版已经很多经典文章了，也推荐了不少经典书籍了，有几个好好看过呢。W.Richard.Stevens的UNP我算是认真看过加了不少旁注，APUE就没有那么认真了，而卷II的一半认真看过，写过读书笔记，卷III就没有看一页。道格拉斯的卷I、卷III是认真看过几遍，卷II就只断续看过。而很多技术文章，如果搞到手了就懒得再看，却不知道这浪费了多少资源，忽略了多少资源。</p>
<!-- more -->
<p>BBS是真正能学到东西的地方吗？rain说不是的，我说也不是的。不过这里能开阔人的视野，能得到对大方向的指引，足够了。我一直都希望大家从这里学到的不是技术本身，而是学习方法和一种不再狂热的 淡然。很多技术，明天就会过时，如果你掌握的是学习方法，那你还有下一个机会，如果你掌握的仅仅是这个技术本身，你就没有机会了。</p>
<!-- more -->
<p>其实我对系统安全是真不懂，因为我一直都喜欢看程序写程序却不喜欢也没有能力攻击谁谁的主机/站点 。我所能在这里做的是，为大家提供一个方向，一种让你的狂热归于淡然的说教。如果你连&lt;&lt;Windows NT设备驱动程序编写&gt;&gt;、&lt;&lt;win9x系统编程&gt;&gt;都没有看过，却要写个什么隐藏自己的木马，搞笑。如果你 看都不看汇编语言，偏要问exploit code的原理，那我无法回答也不想回答你。总有人责问，要讨个说法纭纭，说什么提问却没有回答。不回答已经是正确的处理方式了，至少没有回你一句，看书去，对不对，至少没有扰乱版面让你生闷气。 Unix的man手册你要都看完了，想不会Unix都不行了。微软的MSDN、Platfor SDK DOC你要看完了，你想把Win编程想象得稍微困难点都找不到理由。</p>
<!-- more -->
<p>还是那句话，一个程序员做W.Richard.Stevens那个份上 ，做到逝世后还能叫全世界的顶级hacker们专门著文怀念，但生前却不曾著文攻击，想想看，那是一种什么样的境界，那是一份什么样的淡然。我们可以大肆讨论技术问题，可以就技术问题进行激烈的卓有成效的讨论，却无意进行基础知识、资源信息的版面重复。我刚在前面贴了一堆isbase的文章，开头就是主页标识，却在后面立刻问什么主页在哪里？前面刚刚讨论过如何修改mac地址，后面马上又来一个，前后相差不过3篇文章。选择沉默已经是很多朋友忍耐力的优异表现了。</p>
<!-- more -->
<p>这次sztcww问的关于socket号为什么选择4而不是3，就很有专业精神，虽然我也不清楚，但这样的问题 我就乐意代他请教tt本人，至少他认真看了文章研究过代码，而不是盲目地发问。如此讨论问题的同时 ，大家都可以提高。谁都乐意参与讨论这种问题。很多东西都是可以举一反三的。vertex的lids，被 packetstorm天天追踪更新，你要是看了THC的那三篇，觉得理解一个就理解了一堆，都是内核模块上的 手脚。你不看你怎么知道。我不想在这里陷入具体技术问题的讨论中去，你要是觉得该做点什么了，就自己去看自己去找。 没有什么人摆什么架子，也没有什么人生来就是干这个的。你自己问自己，尽力了吗？</p>
]]></content>
    </entry>
</feed>